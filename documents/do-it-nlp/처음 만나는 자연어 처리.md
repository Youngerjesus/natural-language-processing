# 처음 만나는 자연어 처리 

## 언어모델이 할 수 있는 것
- 문서 분류
- 기계독해
- 문장 생성
- 요약 및 번역

## 기술의 발전에서 핵심을 잃지않으면 흐름을 놓치지 않는다
- Word2vec, transformer, ELMo, Bert, GPT, HyperClova 등의 기술이 등장했다. 하지만 자연어 처리에서 성장을 이끌어낸 것은 transformer 와 transfer learning (전이 학습) 이다.
- 이 두 방아쇠가 당겨진게 GPT 와 Bert 이다.
- 이들을 포함해서 자연어처리를 정석적으로 공부해보려면 CNN 과 RNN 을 살펴봐야한다.
- 더 나아가 선형회귀, 로지스틱 회귀와 여러 수학들에 대해서도 알아야함.

## 자연어 처리 모델이란
- 자연어를 입력으로 받아서 원하는 결과들에 대한 확률을 반환하는 확률 함수.
- 문장 감성 분석 모델은 자연어를 바탕으로 긍정인지, 중립인지, 부정인지에 대한 각각의 확률을 반환.
- 즉 확률이 나오면 이를 후처리해서 원하는 결과(= 자연어 같은 걸로) 반환

## 가장 인기 많은 모델이 딥러닝
- 쓰는 이유는 여러 모델보다 성능이 제일 좋아서.
- 딥이람 hidden layer 를 이용한다는 뜻.
- 딥러닝은 이미지 분류, 음성 인식, 자연어 처리 등에 쓰인다.

## 학습을 위해선 데이터가 있어야한다.
- 지도 학습인듯. 정답이 있는.

## 트랜스퍼 러닝
- 특정 테스크를 학습한 모델을 다른 태스크 수행에 재사용하는 것. 사람으로 치면 이미 알고있는 지식을 이용해서 새로운 지식을 배우는 것.
- 이미 데이터1로 학습한 모델을 이용해서 데터2를 가지고 학습시키는 것
- 모델의 학습 속도나 모델의 성능이 더 잘되는 경향이 있다. BERT 나 GPT 도 이 학습을 이용했음.
- 본격적인 일을 하기 전에 테스크를 학습하능 과정을 업스트림 테스크하고, 본격적인 테스크를 수행하기 위해서 학습된 모델을 이용해서 학습하는 걸 다운스트림 테스크라고 한다.
- 업스트림 테스크를 학습하는 과정을 pretrain 이라고 한다.

## 업스트림테스크
- 자연어의 풍부한 문맥을 이해시킨 모델을 만들어 놓아서 복잡한 일을 더 잘 수행하도록 만들 수 있는 기법
- 대표적인 업스트림 테스크가 다음 단어 맞추기. 이 방법으로 업스트림 테스크를 수행한 모델을 언어 모델이라고 한다.
- 또 다른 업스트림 테스크가 빈칸 맞추기가 있다. BERT 가 이방법으로 학습을 시켜놓음.
- 빈칸 채우기로 학습한 모델을 마스크 언어 모델이라고 한다.
- 아 이런 언어모델은 지도 학습할 때 비용이 작다. 그냥 백과사전, 뉴스, 웹문서만 던저주면 되는거라서. 이처럼 데이터내에서 정답을 만들고 학습하는 걸 자가지도학습(self-supervised learning) 이라고한다. 지도학습보다 비용이싸다.

## 다운스트림 태스크
- 업스트림 테스크를 한 목적 자체는 다운 스트림을 질하기 위해서다.
- 업스트림테스크를 마친 모델을 그대로 쓰거나 몇 개 테스크를 더 거친후에 쓴다.
- 다운스트림 테스크의 본질은 분류(classification) 이다. 자연어를 입력받아서 이게 어떤 범주에 해당하는지 확률을 생성한다
- 문장 생성을 제외한 대부분의 테스크는 프리트레인이 끝난 마스크언어모델을 쓴다

## 파인 튜닝
- 프리트레인을 마친 모델을 다운스트림 태스크에 맞게 가공하는 것
- 예시는 문서 분류, 자연어추론, 개체명인식, 질의응답, 문장생성이 있다

## 문서 분류
- 자연어를 입력받아 해당 문장이 어떤 범주에 속하는지 판단하는 것. 긍정, 부정, 중립 등

## 자연어추론
- 두 문장의 관계를 설명. 진실, 거짓 등

## 개체명인식
- 단어가 어떤 개체에 속하는지 설명하는 것 기관이나 지명 등

## 질의응답
- 질문을 입력받아서 해당 단어가 답변의 시작에 속하는지 중간에 속하는지 끝에 속하는지에 대한 확률을 반환

## 문장생성
- GPT 계열 언어모델이 주로 씀. 자연어를 입력받아서 어휘 전체에대한 확률값 반환.

## 파인 튜닝 이외의 방법도 있다
- 파인 튜닝은 다운스트림 테스크 데이터를 이용해서 모델 전체를 업데이트한다. 이는 비용이 많이들어서 모델 일부만 업데이트 하는 프롬프트 튜닝 (Prompt Tuning) 도 있고 다운스트림 테스크 데이터 일부만 쓰고 모델을 업데이트 하지 않는 인컨택스트 러닝 (incontext learning) 도 있다

# #인컨택스트 러닝
- 여기에 또 여러가지 용어가 있음
- 제로샷 러닝: 데이터참고 없이 다운스트림 테스크를 수행해보는 것.
- 원샷 러닝: 다운스트림 데이터 한 건만 써보는 것
- 퓨삿 러닝 (few-shot learning): 데이터 몇 건만 써보는 것


## etc) 자연어처리로 할 수 있을만한 일들
- 네이버 영화평 오픈소스 말뭉치 기반으로 동의어들을 생성해본다면?
- 이미 잘 만들어진 모델을 가지고 동의어를 생성하는 데이터로 학습을 시킨다면?

## 학습 파이프라인 

1) 각종 설정값 정하기
- 어떤 pretrain 모델을 사용할 지
- 학습에 사용할 데이터는 뭘로할지
- 학습 결과는 어디에다가 저장할지
- 하이퍼파라미터는 뭘로 정할지 
  - learning rate, 
  - batch size

2) 데이터 내려받기

- 여기서는 pretrain 을 마친 모델을 다운스트림 테스크를 위해서 파인 튜닝하는 실습을 한다. 
- 그러기 위해서 파인 튜닝에 필요한 데이터를 내려받아야한다.

3) 프리트레인 마친 모델 준비하기

- 프리트레인에는 많은 리소스가 필요로한다. 근데 그렇게 걱정하지 않아도 되는게 개인과 기업들이 프리트레인을 마친 모델을 자유롭게 사용하도록 공개하고 있어서 그렇게 큰 걱정하지 않아도 된다. 
- 여기서는 BERT 나 GPT 같은 트랜스포머 계열 모델로 실습한다.

4) 토크나이저 준비하기 

- 자연어처리 모델의 입력은 대게 토큰이다. 문장을 토큰 시퀀스로 분석하는 과정을 토큰화라고 하고, 토큰화를 수행하는 프로그램을 토크나이저라고 한다.

5) 데이터로더 준비하기 

- 파이토치로 딥러닝을 하려면 데이터로더가 있어야한다. 
- 파이토치는 딥러닝 모델 학습을 지원하는 라이브러리가 포함되어있다. 
- 데이터로더는 데이터를 배치 형태로 모델에 밀어넣어주는 역할을 한다. 전체 데이터 가운데 일부만 뽑아서 배치로 만듦.

6) 테스크 정의하기 

- task 정의는 모델의 최적화 방법과 학습 과정 정의하는 것

7) 모델 학습하기 

- 트레이너는 실제 학습을 수행하는 역할을 담당. GPU 등 하드웨어 설정, 학습 기록 로깅, 체크 포인트 저장등 복잡한 과정을 대신 수행해줌.
